<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>17th Workshop on Virtualization in High-Performance Cloud Computing</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022<p><center><table style=&#34;width: 100%;&#34;><tr><td style=&#34;width: 20%;&#34;><a href=&#34;../&#34; style=&#34;color: white;&#34;>[Back]</a></td></tr></table></center>"><meta name=generator content="Hugo 0.68.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content><meta property="og:description" content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022<p><center><table style=&#34;width: 100%;&#34;><tr><td style=&#34;width: 20%;&#34;><a href=&#34;../&#34; style=&#34;color: white;&#34;>[Back]</a></td></tr></table></center>"><meta property="og:type" content="article"><meta property="og:url" content="https://vhpc.github.io/program-details-2022/"><meta property="og:image" content="https://vhpc.github.io/images/featured.png"><meta property="og:site_name" content="17th Workshop on Virtualization in High-Performance Cloud Computing"><meta itemprop=name content><meta itemprop=description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022<p><center><table style=&#34;width: 100%;&#34;><tr><td style=&#34;width: 20%;&#34;><a href=&#34;../&#34; style=&#34;color: white;&#34;>[Back]</a></td></tr></table></center>"><meta itemprop=wordCount content="2975"><meta itemprop=image content="https://vhpc.github.io/images/featured.png"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://vhpc.github.io/images/featured.png"><meta name=twitter:title content><meta name=twitter:description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022<p><center><table style=&#34;width: 100%;&#34;><tr><td style=&#34;width: 20%;&#34;><a href=&#34;../&#34; style=&#34;color: white;&#34;>[Back]</a></td></tr></table></center>"></head><body class="ma0 avenir bg-light-gray"><header class="cover bg-top" style=background-image:url(https://vhpc.github.io/images/featured.png)><div class="pb3-m pb6-l bg-black-60"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib"><img src=/images/vhpclogo-yellow.png class="w100 mw5-ns" alt="17th Workshop on Virtualization in High-Performance Cloud Computing"></a><div class="flex-l items-center"><div class=ananke-socials><a href=https://twitter.com/VHPC3 target=_blank class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel=noopener aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:{{ .fill }}"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" ><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></div></div></div></nav><div class="tc-l pv4 pv6-l ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">17th Workshop on Virtualization in High-Performance Cloud Computing</h1><h2 class="fw1 f5 f3-l white-80 measure-wide-l center mt3">held in conjunction with <a href=https://www.isc-hpc.com style=color:#fff>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022<p><center><table style=width:100%><tr><td style=width:20%><a href=../ style=color:#fff>[Back]</a></td></tr></table></center></h2></div></div></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns tj-l mw9 lh-copy"><header><p class="f6 b helvetica tracked"></p><h1 class=f1></h1></header><div class="nested-copy-line-height lh-copy f4 nested-links nested-img mw9 tj-l mid-gray"><h2 id=vhpc-2022-program-details>VHPC 2022 Program Details</h2><h3 id=keynote-talks>Keynote Talks</h3><h4 id=rtla-finding-the-sources-of-os-noise-on-linux><strong>rtla: finding the sources of OS noise on Linux</strong></h4><p><em>Speaker</em>: <a href=https://bristot.me>Daniel Bristot De Oliveira</a>, Senior Principal Software Engineer in the real-time kernel team at <a href=https://redhat.com/>Red Hat</a>.</p><p><em>Abstract</em>: Currently, Real-time Linux is evaluated using a black-box approach. While this method provides an overview of the system, it fails to provide a root cause analysis for unexpected values. Developers have to use kernel trace features to debug these cases, requiring extensive knowledge about the system and fastidious tracing setup and breakdown. Such analysis will be even more impactful after the PREEMPT_RT merge. To support these cases, since version 5.17, the Linux kernel includes a new tool named rtla, which stands for Real-time Linux Analysis. The rtla is a meta-tool that consists of a set of commands that aims to analyze the real-time properties of Linux. Instead of testing Linux as a black box, rtla leverages kernel tracing capabilities to provide precise information about latencies and root causes of unexpected results. In this talk, Daniel will present two tools provided by rtla. The timerlat tool to measure IRQ and thread latency for interrupt-driven applications and the osnoise tool to evaluate the ability of Linux to isolate workload from the interferences from the rest of the system. The presentation includes examples of how to use the tool to find the root cause analysis and collect extra tracing information directly from the tool.</p><h4 id=dynamodb-nosql-database-services-for-predictable-hpc-workloads><strong>DynamoDB: NoSQL database services for predictable HPC workloads</strong></h4><p><em>Speakers</em>: <a href=https://www.linkedin.com/in/akshatvig/>Akshat Vig</a>, Principal Software Engineer at <a href=https://aws.amazon.com>AWS</a> and <a href=https://www.linkedin.com/in/amit-purohit-9b4a833/>Amit Purohit</a>, Director/General Manager of Database Services at <a href=https://aws.amazon.com>AWS</a>.</p><p><em>Abstract</em>: Data is growing at exponential pace and organizations are using data to make effective decisions, where one of the key challenges is making them &ldquo;fast&rdquo;. In this talk, we will look at how AWS is continuously innovating on behalf of the customers to make it easy to access data and build data-driven applications in the cloud. We will also look at the journey of what we learned as we built Amazon DynamoDB, one of the NoSQL data stores in AWS used by hundreds of thousands of customers providing consistent performance at any scale. As DynamoDB has evolved over the last ten years, it has faced numerous challenges in delivering newer offerings such as Indexing, Encryption, Backups, Streams and Global tables without sacrificing predictable performance. We will share the lessons learned and how we evolved the service continuously by improving its performance characteristics while adding new features.</p><h3 id=accepted-paper-presentations>Accepted Paper Presentations</h3><h4 id=ebpf-based-extensible-paravirtualization><strong>eBPF-based Extensible Paravirtualization</strong></h4><p><em>Authors</em>: Luigi Leonardi, Giuseppe Lettieri, Giacomo Pellicci (University of Pisa)</p><p><em>Abstract</em>: High performance applications usually need to give many hints to the OS kernel regarding their needs. For example, CPU affinity is commonly used to pin processes to cores and avoid the cost of CPU migration, isolate performance critical tasks, bring related tasks together, and so on. However, when running inside a Virtual Machine, the (guest) OS kernel can only assign virtual resources, e.g., pinning a guest process to a virtual CPU thread (vCPU); the vCPU thread, however, is still freely scheduled by the host hypervisor, which is unaware of the guest application requests. This semantic gap is usually overcome by statically allocating virtual resources to their hardware counterparts, which is costly and inflexible, or via paravirtualization, i.e., by modifying the guest kernel to pass the hints to the host, which is cumbersome and difficult to extend. We propose to use host-injected eBPF programs as a way for the host to obtain this kind of information from the guest in an extensible way, without modifying the guest (Linux) kernel, and without statically allocating resources. We apply this idea to the example of CPU affinity and run some experiments to show its effect on several microbenchmarks. Finally, we discuss the implications for confidential computing.</p><h4 id=virtual-clusters-isolated-containerized-hpc-environments-in-kubernetes><strong>Virtual Clusters: Isolated, Containerized HPC Environments in Kubernetes</strong></h4><p><em>Authors</em>: George Zervas, Anthony Chazapis, Yannis Sfakianakis, Christos Kozanitis, Angelos Bilas (FORTH & University of Crete)</p><p><em>Abstract</em>: Today, Cloud and HPC workloads tend to use different approaches for managing resources. However, as more and more applications require a mixture of both high-performance and data processing computation, convergence of Cloud and HPC resource management is becoming a necessity. Cloud-oriented resource management strives to share physical resources across applications to improve infrastructure efficiency. On the other hand, the HPC community prefers to rely on job queueing mechanisms to coordinate among tasks, favoring dedicated use of physical resources by each application. In this paper, we design a combined Slurm-Kubernetes system that is able to run unmodified HPC workloads under Kubernetes, alongside other, non-HPC applications. First, we containerize the whole HPC execution environment into a virtual cluster, giving each user a private HPC context, with common libraries and utilities built-in, like the Slurm job scheduler. Second, we design a custom Slurm-Kubernetes protocol that allows Slurm to dynamically request resources from Kubernetes. Essentially, in our system the Slurm controller delegates placement and scheduling decisions to Kubernetes, thus establishing a centralized resource management endpoint for all available resources. Third, our custom Kubernetes scheduler applies different placement policies depending on the workload type. We evaluate the performance of our system compared to a native Slurm-based HPC cluster and demonstrate its ability to allow the joint execution of applications with seemingly conflicting requirements on the same infrastructure with minimal interference.</p><h4 id=on-the-use-of-linux-real-time-features-for-ran-packet-processing-in-cloud-environments><strong>On the use of Linux Real-Time Features for RAN Packet Processing in Cloud Environments</strong></h4><p><em>Authors</em>: Luca Abeni, Tommaso Cucinotta, Balazs Pinczel, Peter Matray, Murali Srinivasan, Tobias Lindquist (Scuola Superiore Sant&rsquo;Anna & Ericsson)</p><p><em>Abstract</em>: This paper shows how to use a Linux-based operating system as a real-time processing platform for low-latency and predictable packet processing in cloudified radio-access network (cRAN) scenarios. This use-case exhibits challenging end-to-end processing latencies, in the order of milliseconds for the most time-critical layers of the stack. A significant portion of the variability and instability in the observed end-to-end performance in this domain is due to the power saving capabilities of modern CPUs, often in contrast with the low-latency and high-performance requirements of this type of applications. We discuss how to properly configure the system for this scenario, and evaluate the proposed configuration on a synthetic application designed to mimic the behavior and computational requirements of typical software components implementing baseband processing in production environments.</p><h4 id=analyzing-unikernel-support-for-hpc-experimental-study-of-openmp><strong>Analyzing Unikernel Support for HPC: Experimental Study of OpenMP</strong></h4><p><em>Author</em>: Abdulrahman Azab (University of Oslo & Mansoura University)</p><p><em>Abstract</em>: Unikernels are single-application operating systems designed to run as virtual machines. They are popular in the cloud domain and are considered as a good alternative to containers due to the benefits they provide in terms of performance, low resource consumption, and security. This paper investigates the use of unikernels as a platform for HPC applications, considering both the potential advantages as well as the limitations of this novel OS model. The performance and stability of two unikernel platforms (HermitCore and HermiTux) are experimen- tally evaluated over standard representative HPC OpenMP benchmarks. We observe that unikernels remarkably reduce the overhead due to sys- tem calls, leading to a significant speedup (up to 77%) in system-bound applications. For applications that are not system-intensive, there are a few performance differences between the unikernel and the vanilla Linux execution. It should be remarked that modern unikernel projects are not yet fully mature, and exhibit stability issues running some OpenMP benchmarks.</p><h3 id=accepted-lightning-talks>Accepted Lightning Talks</h3><h4 id=torov-a-kernel-in-user-space-to-deploy-server-less-applications><strong>ToroV, a kernel in user-space to deploy server-less applications</strong></h4><p><em>Speaker</em>: Matias Vara (Vates SAS)</p><p><em>Abstract</em>: Current cloud providers allow to better make use of resources like memory or CPU by enabling the deployment of server-less applications. These are lightweight applications that execute for a short period of time. A single physical host can be used to deploy thousands of sever-less applications. To increase the number of applications deployed per host, the deployment of sever-less applications requires to be efficient in terms of host resources like memory and CPU. Cloud providers are interested to reduce the required footprint. Resources per application are limited to prevent one single deployment from drying all the resources. Also, server-less applications require strong isolation to avoid interfering with other applications that run in the same host. To achieved these requirements, sever-less applications can be deployed by using Virtual Machines, containerized processes, or a combination of them. This paper proposes to deploy server-less applications by relying on a Virtual Machine Monitor name ToroV that exposes a POSIX API to the application. ToroV intercepts these hypercalls, servers them, and returns to the application. ToroV runs in the host user-space and acts as a standard kernel by providing networking and filesystem services. The user configures the syscalls that the application is allowed to invoke thus reducing the potential host&rsquo;s attack surface. This paper presents the design of ToroV together with its implementation. We evaluate our approach in terms of performance and safety and we compare it with existing approaches like containers, microVMs and gVisor.</p><h4 id=keep-an-eye-on-your-busy-waiting><strong>Keep an eye on your busy-waiting</strong></h4><p><em>Speaker</em>: Remo Andreoli (Scuola Superiore Sant&rsquo;Anna)</p><p><em>Authors</em>: Remo Andreoli, Tommaso Cucinotta (Scuola Superiore Sant&rsquo;Anna), Daniel Bristot De Oliveira (Red Hat)</p><p><em>Abstract</em>: The predominant need for highly scalable cloud infrastructures, capable of ingesting the ever-growing volume, high-throughput requirement and extreme heterogeneity of nowadays&rsquo; data-driven applications, has seen the emergence of optimized/lightweight software stack for HPC workloads. A common technique employed by these high-performance libraries to increase throughput is the so-called kernel bypass: busy-waiting to avoid context switches and to prevent CPU&rsquo;s power-saving features from activating, thus minimizing the OS overhead. Although ``spinning&rdquo; proves to be beneficial in certain contexts, it requires special care to avoid wasting resources and counter-intuitive behaviours. In this talk, we will address an instance of un-safe busy-waiting in WiredTiger, the underlying storage engine of MongoDB, which led to a 328% increase in worst-case latency.</p><h4 id=unified-workload-deployment-on-heterogeneous-hpc-clusters><strong>Unified workload deployment on heterogeneous HPC clusters</strong></h4><p><em>Speaker</em>: Anastassios Nanos (Nubificus LTD)</p><p><em>Authors</em>: Anastassios Nanos, George Ntoutsos, Charalampos Mainas (Nubificus LTD)</p><p><em>Abstract</em>: Application deployment in current cloud computing environments follows the Platform-, Software-, and Function-as-a-Service paradigm (PaaS, SaaS, FaaS). These approaches offer performance and flexibility improvements over Infrastructure-as-a-Service by decoupling the application from the infrastructure. Managing the application lifecycle in such environments is handled using high-level orchestrators such as Kubernetes (k8s), a container orchestration framework. To deploy a workload on a k8s cluster all the user has to do is build a container image, and post an API request to the k8s API. In the HPC context, however, hardware resources are significantly different from generic cloud infrastructure: custom interconnects/CPU architectures, hardware accelerators etc. A container image, inherently, is built to be able to run on generic hardware. We build on the OCI image specification and propose a new abstraction, based on the &ldquo;image index&rdquo; and &ldquo;manifest&rdquo; features. Coupled with a respective OCI-compatible runtime, an HPC application can be efficiently deployed on an HPC k8s cluster, taking into account the specific hardware resources available on the node where it will be scheduled.</p><h3 id=invited-talks>Invited Talks</h3><h4 id=mobility-and-elasiticity-in-high-performace-cloud-computing><strong>Mobility and Elasiticity in High Performace Cloud Computing</strong></h4><p><em>Speaker</em>: Eric Jul (University of Oslo)</p><p><em>Abstract</em>: In this talk, we start by taking a look at data and computation mobility from a historic perspective, from early UNIX process based migration to modern-day container mobility in High Performance Cloud Computing. Mobility has been proposed and implemented in distributed systems in various ways since the 1980&rsquo;ies, but most of the early work has not seen wide-spread use, as mobility often appeared as a solution in search of a problem. That changed with the advent of Cloud Computing where useful applications of mobility in Cloud Computing data centers appeared, useful for load balancing, dynamic hardware upgrades, and elasticity of the services provided. The talk will present both the usage and the underlying techniques to achieve high performance and elasticity in modern Cloud Computing infrastructures.</p><h4 id=platform-level-resource-orchestration-and-abstraction-layers-in-continuum-platforms-the-challenge-of-heterogeneity><strong>Platform-level resource orchestration and abstraction layers in continuum platforms: the challenge of heterogeneity</strong></h4><p><em>Speaker</em>: Massimo Coppola (ISTI, CNR)</p><p><em>Abstract</em>: The talk will provide an overview of the approach to Continuum computing platforms brought on by the current research projects Accordion and TEACHING. Next-Generation applications call for a combination of computation, networking and storage-related QoS constraints that need to be guaranteed in spite of the volatility and changing behavior of all types of resources, of their heterogeneity, and of the dynamically changing mobility and needs of end-users. We&rsquo;ll discuss the approach to dynamic management of resources in platforms that encompass Clouds, Near- and Far-Edge devices, mobile and IoT devices, entailing the exploitation of different virtualization abstractions such as VMs, containers and unikernels, as well as the use of learning and AI techniques in order to characterize resource behavior.</p><h4 id=scalable-containers-in-hpc-and-the-convergence-toward-hpc-clouds><strong>Scalable containers in HPC and the convergence toward HPC clouds</strong></h4><p><em>Speaker</em>: Azat Khuziyakhmetov (GWDG)</p><p><em>Abstract</em>: The installation and configuration of applications in HPC is highly dependent on the actual software and hardware environment that users find on the HPC systems of the data centers (OS distribution, libraries, modules, network, architectures). This usually complicates the version provisioning of the software as well as migration between data centers. Both are usually associated with a certain support effort. When moving to cloud resources (for instance, to offer applications as on-demand services) or to user’s hardware (for small tests and development) the software environment must be reproduced manually. The NHR project &ldquo;Containers and Container Management&rdquo; aims to mitigate these issues and offer a solution for users, also considering possible performance degradation due to different hardware used for building software within the container and running it. We exploit the findings from the project surveys and evaluate how they could be used for spawning containers in cloud like environments utilizing the existing HPC resources (HPC-as-a-Service) or to augment them, for example to provide development and testing resources on cloud platforms. This requires implementing container spawning options for the HPC schedulers transparently on HPC and on cloud like resources as well as a mechanism for building containers that provide the same software environment as the HPC systems.</p><h4 id=virtualisation-overhead-analyzed-using-io500><strong>Virtualisation Overhead Analyzed Using IO500</strong></h4><p><em>Speaker</em>: Julian Kunkel (GWDG)</p><p><em>Abstract</em>: We compare I/O performance of bare-metal services, using Singularity and OpenStack System using the IO500 benchmark on various configurations. Additionally, the overhead of using LUKS-encrypted storage is discussed.</p><h4 id=enroot-and-pyxis--lightweight-unprivileged-containers-for-slurm-clusters><strong>Enroot and Pyxis – Lightweight, Unprivileged Containers for Slurm Clusters</strong></h4><p><em>Speaker</em>: Luke Yeager (NVIDIA)</p><p><em>Abstract</em>: HPC cluster users are increasingly interested in containerizing their applications, but administrators may be justifiably wary of installing a container runtime with a large codebase that is difficult to audit and which comes with a checkered history of security issues. We present our solution to this problem. Enroot is a simple yet powerful tool to turn traditional container images into unprivileged sandboxes. It was designed with HPC clusters in mind – truly unprivileged, with a small codebase that is easy to audit, and providing only the minimum features required. Crucially, it provides only minimal isolation from the host OS - it does not provide a network namespace or any other features which are unnecessary for HPC workloads, and which can lead to virtualization overheads and bloated code. In addition, we present pyxis – a plugin for Slurm which integrates enroot smoothly into Slurm command-line utilities. Together, enroot and pyxis provide an easy way for users to get started with containers, without compromising either application performance or cluster security.</p><h4 id=mpi-based-graph-computing-with-serverless-fault-tolerance-in-clouds><strong>MPI-based Graph Computing with Serverless Fault Tolerance in Clouds</strong></h4><p><em>Speakers</em>: Peng Lin and Lin Ma (ByteDance)</p><p><em>Abstract</em>: Graph processing aims to process super large-scale graphs and execute graph data analysis tasks, e.g., PageRank, TriangleCount, community detection, etc. Most graph processing systems are under in-memory architecture, which makes it hard to process real-world large graphs with gigantic number of edges (e.g., 100 trillion edges) at a cheap cost. In addition, few graph processing systems really implement serverless fault tolerance capability, hindering wide adoption in production. Hence, we proposed an enterprise-level graph analytics platform for graph computing and graph mining in ByteDance. The system is built on a serverless engine atop Kubernetes to provide flexible cluster resource management, automatic deployment in cloud, elasticity for scalability and fault tolerance. It eases the maintenance and deployment effort, and lowers the number of machines and memory consumption but supports much larger graphs by a hierarchical out-of-core design of DRAM/PMEM/SSDs on Kubernetes. We also have built an actor-based control plane for rank management and stateful fault tolerance as an infrastructure component. It handles failures at container level, agent level, and worker level end-to-end with synergy that MPI can not fully cover. The agents of dynamically-assigned ranks manage worker processes of different languages as well as checkpoints in PMEM. It can relaunch agents and workers of specific ranks, load mutable status of vertices from vertex table in PMEM, and ensure that they are automatically recoverable from any iteration for any workers of arbitrary ranks.</p><h4 id=state-of-kubernetes-in-container-orchestration-for-scientific-computing><strong>State of Kubernetes in Container Orchestration for Scientific Computing</strong></h4><p><em>Speaker</em>: Michael Alexander (BOKU Vienna and OeAW, Austria)</p><p><em>Abstract</em>: Kubernetes is finding wide-spread adoption as a container orchestrator for predominatly stateless service-oriented workloads. Given its affinity with large scale-out, it is being looked at as a platform for scientific workloads. This interactive talk looks at the applicability of processing models including pipeline, serverless and batch. Application containerization and CI/CD aspects are highlighted for discussion. The talk highlights some current Kubernetes HPC coupling proposals such as an operator for the Torque scheduler or Singularity as a K8s container runtime. Issues including data locality/placement are pointed out.</p><h3 id=general-information>General Information</h3><p>The <a href=https://vhpc.org>17th Workshop on Virtualization in High­-Performance Cloud Computing (VHPC 2022)</a> aims
to bring together researchers and industrial practitioners facing the
challenges posed by virtualization in HPC/Cloud scenarios
in order to foster discussion,
collaboration, mutual exchange of knowledge and experience, enabling research
to ultimately provide novel solutions for virtualized computing systems of
tomorrow.</p><p>The workshop will have a duration of one day and it will be held in conjunction with the
<a href=https://www.isc-hpc.com>International Supercomputing Conference - High Performance (ISC) 2022</a> on June 2nd, 2022 in
Hamburg, Germany.</p><p>For more information, refer to either the <a href=https://vhpc.org>VHPC 2022</a> or the <a href=https://www.isc-hpc.com>ISC 2022</a> web pages.</p></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://vhpc.github.io>&copy; 17th Workshop on Virtualization in High-Performance Cloud Computing 2023</a><div><div class=ananke-socials><a href=https://twitter.com/VHPC3 target=_blank class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel=noopener aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:{{ .fill }}"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" ><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></div></div></div></footer></body></html>