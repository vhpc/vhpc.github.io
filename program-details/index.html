<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>17th Workshop on Virtualization in High-Performance Cloud Computing</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022"><meta name=generator content="Hugo 0.68.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content><meta property="og:description" content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022"><meta property="og:type" content="article"><meta property="og:url" content="https://vhpc.github.io/program-details/"><meta property="og:image" content="https://vhpc.github.io/images/featured.png"><meta property="og:site_name" content="17th Workshop on Virtualization in High-Performance Cloud Computing"><meta itemprop=name content><meta itemprop=description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022"><meta itemprop=wordCount content="1743"><meta itemprop=image content="https://vhpc.github.io/images/featured.png"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://vhpc.github.io/images/featured.png"><meta name=twitter:title content><meta name=twitter:description content="held in conjunction with <a href=&#34;https://www.isc-hpc.com&#34; style=&#34;color: white;&#34;>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022"></head><body class="ma0 avenir bg-light-gray"><header class="cover bg-top" style=background-image:url(https://vhpc.github.io/images/featured.png)><div class="pb3-m pb6-l bg-black-60"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib"><img src=/images/vhpclogo-yellow.png class="w100 mw5-ns" alt="17th Workshop on Virtualization in High-Performance Cloud Computing"></a><div class="flex-l items-center"><div class=ananke-socials><a href=https://twitter.com/VHPC3 target=_blank class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel=noopener aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:{{ .fill }}"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" ><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></div></div></div></nav><div class="tc-l pv4 pv6-l ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">17th Workshop on Virtualization in High-Performance Cloud Computing</h1><h2 class="fw1 f5 f3-l white-80 measure-wide-l center mt3">held in conjunction with <a href=https://www.isc-hpc.com style=color:#fff>ISC-HPC</a>, Hamburg, Germany, May 29th - Jun 2nd 2022</h2></div></div></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns tj-l mw9 lh-copy"><header><p class="f6 b helvetica tracked"></p><h1 class=f1></h1></header><div class="nested-copy-line-height lh-copy f4 nested-links nested-img mw9 tj-l mid-gray"><h2 id=vhpc-2022-program-details>VHPC 2022 Program Details</h2><p>(schedule to be defined soon)</p><h3 id=keynote-talks>Keynote Talks</h3><h4 id=rtla-finding-the-sources-of-os-noise-on-linux><strong>rtla: finding the sources of OS noise on Linux</strong></h4><p><a href=https://bristot.me>Daniel Bristot De Oliveira</a>, Senior Principal Software Engineer in the real-time kernel team at <a href=https://redhat.com/>Red Hat</a>.</p><blockquote><p>Currently, Real-time Linux is evaluated using a black-box approach. While this method provides an overview of the system, it fails to provide a root cause analysis for unexpected values. Developers have to use kernel trace features to debug these cases, requiring extensive knowledge about the system and fastidious tracing setup and breakdown. Such analysis will be even more impactful after the PREEMPT_RT merge. To support these cases, since version 5.17, the Linux kernel includes a new tool named rtla, which stands for Real-time Linux Analysis. The rtla is a meta-tool that consists of a set of commands that aims to analyze the real-time properties of Linux. Instead of testing Linux as a black box, rtla leverages kernel tracing capabilities to provide precise information about latencies and root causes of unexpected results. In this talk, Daniel will present two tools provided by rtla. The timerlat tool to measure IRQ and thread latency for interrupt-driven applications and the osnoise tool to evaluate the ability of Linux to isolate workload from the interferences from the rest of the system. The presentation includes examples of how to use the tool to find the root cause analysis and collect extra tracing information directly from the tool.</p></blockquote><h4 id=dynamodb-nosql-database-services-for-predictable-hpc-workloads><strong>DynamoDB: NoSQL database services for predictable HPC workloads</strong></h4><p><a href=https://www.linkedin.com/in/akshatvig/>Akshat Vig</a>, Principal Software Engineer at <a href=https://aws.amazon.com>AWS</a> and <a href=https://www.linkedin.com/in/amit-purohit-9b4a833/>Amit Purohit</a>, Director of Database Services at <a href=https://aws.amazon.com>AWS</a>.</p><h3 id=accepted-paper-presentations>Accepted Paper Presentations</h3><h4 id=ebpf-based-extensible-paravirtualization><strong>eBPF-based Extensible Paravirtualization</strong></h4><p>Luigi Leonardi, Giuseppe Lettieri, Giacomo Pellicci (University of Pisa)</p><blockquote><p>High performance applications usually need to give many hints to the OS kernel regarding their needs. For example, CPU affinity is commonly used to pin processes to cores and avoid the cost of CPU migration, isolate performance critical tasks, bring related tasks together, and so on. However, when running inside a Virtual Machine, the (guest) OS kernel can only assign virtual resources, e.g., pinning a guest process to a virtual CPU thread (vCPU); the vCPU thread, however, is still freely scheduled by the host hypervisor, which is unaware of the guest application requests. This semantic gap is usually overcome by statically allocating virtual resources to their hardware counterparts, which is costly and inflexible, or via paravirtualization, i.e., by modifying the guest kernel to pass the hints to the host, which is cumbersome and difficult to extend. We propose to use host-injected eBPF programs as a way for the host to obtain this kind of information from the guest in an extensible way, without modifying the guest (Linux) kernel, and without statically allocating resources. We apply this idea to the example of CPU affinity and run some experiments to show its effect on several microbenchmarks. Finally, we discuss the implications for confidential computing.</p></blockquote><h4 id=virtual-clusters-isolated-containerized-hpc-environments-in-kubernetes><strong>Virtual Clusters: Isolated, Containerized HPC Environments in Kubernetes</strong></h4><p>George Zervas, Anthony Chazapis, Yannis Sfakianakis, Christos Kozanitis, Angelos Bilas (FORTH & University of Crete)</p><blockquote><p>Today, Cloud and HPC workloads tend to use different approaches for managing resources. However, as more and more applications require a mixture of both high-performance and data processing computation, convergence of Cloud and HPC resource management is becoming a necessity. Cloud-oriented resource management strives to share physical resources across applications to improve infrastructure efficiency. On the other hand, the HPC community prefers to rely on job queueing mechanisms to coordinate among tasks, favoring dedicated use of physical resources by each application. In this paper, we design a combined Slurm-Kubernetes system that is able to run unmodified HPC workloads under Kubernetes, alongside other, non-HPC applications. First, we containerize the whole HPC execution environment into a virtual cluster, giving each user a private HPC context, with common libraries and utilities built-in, like the Slurm job scheduler. Second, we design a custom Slurm-Kubernetes protocol that allows Slurm to dynamically request resources from Kubernetes. Essentially, in our system the Slurm controller delegates placement and scheduling decisions to Kubernetes, thus establishing a centralized resource management endpoint for all available resources. Third, our custom Kubernetes scheduler applies different placement policies depending on the workload type. We evaluate the performance of our system compared to a native Slurm-based HPC cluster and demonstrate its ability to allow the joint execution of applications with seemingly conflicting requirements on the same infrastructure with minimal interference.</p></blockquote><h4 id=on-the-use-of-linux-real-time-features-for-ran-packet-processing-in-cloud-environments><strong>On the use of Linux Real-Time Features for RAN Packet Processing in Cloud Environments</strong></h4><p>Luca Abeni, Tommaso Cucinotta, Balazs Pinczel, Peter Matray, Murali Srinivasan, Tobias Lindquist (Scuola Superiore Sant&rsquo;Anna & Ericsson)</p><blockquote><p>This paper shows how to use a Linux-based operating system as a real-time processing platform for low-latency and predictable packet processing in cloudified radio-access network (cRAN) scenarios. This use-case exhibits challenging end-to-end processing latencies, in the order of milliseconds for the most time-critical layers of the stack. A significant portion of the variability and instability in the observed end-to-end performance in this domain is due to the power saving capabilities of modern CPUs, often in contrast with the low-latency and high-performance requirements of this type of applications. We discuss how to properly configure the system for this scenario, and evaluate the proposed configuration on a synthetic application designed to mimic the behavior and computational requirements of typical software components implementing baseband processing in production environments.</p></blockquote><h4 id=analyzing-unikernel-support-for-hpc-experimental-study-of-openmp><strong>Analyzing Unikernel Support for HPC: Experimental Study of OpenMP</strong></h4><p>Abdulrahman Azab (University of Oslo & Mansoura University)</p><blockquote><p>Unikernels are single-application operating systems designed to run as virtual machines. They are popular in the cloud domain and are considered as a good alternative to containers due to the benefits they provide in terms of performance, low resource consumption, and security. This paper investigates the use of unikernels as a platform for HPC applications, considering both the potential advantages as well as the limitations of this novel OS model. The performance and stability of two unikernel platforms (HermitCore and HermiTux) are experimen- tally evaluated over standard representative HPC OpenMP benchmarks. We observe that unikernels remarkably reduce the overhead due to sys- tem calls, leading to a significant speedup (up to 77%) in system-bound applications. For applications that are not system-intensive, there are a few performance differences between the unikernel and the vanilla Linux execution. It should be remarked that modern unikernel projects are not yet fully mature, and exhibit stability issues running some OpenMP benchmarks.</p></blockquote><h3 id=accepted-lightning-talks>Accepted Lightning Talks</h3><h4 id=torov-a-kernel-in-user-space-to-deploy-server-less-applications><strong>ToroV, a kernel in user-space to deploy server-less applications</strong></h4><p>Matias Vara (Vates SAS)</p><h4 id=keep-an-eye-on-your-busy-waiting><strong>Keep an eye on your busy-waiting</strong></h4><p>Remo Andreoli (Scuola Superiore Sant&rsquo;Anna)</p><h4 id=unified-workload-deployment-on-heterogeneous-hpc-clusters><strong>Unified workload deployment on heterogeneous HPC clusters</strong></h4><p>Anastassios Nanos, George Ntoutsos, Charalampos Mainas (Nubificus LTD)</p><h3 id=invited-talks>Invited Talks</h3><h4 id=mobility-and-elasiticity-in-high-performace-cloud-computing><strong>Mobility and Elasiticity in High Performace Cloud Computing</strong></h4><p>Eric Jul (University of Oslo)</p><blockquote><p>In this talk, we start by taking a look at data and computation mobility from a historic perspective, from early UNIX process based migration to modern-day container mobility in High Performance Cloud Computing. Mobility has been proposed and implemented in distributed systems in various ways since the 1980&rsquo;ies, but most of the early work has not seen wide-spread use, as mobility often appeared as a solution in search of a problem. That changed with the advent of Cloud Computing where useful applications of mobility in Cloud Computing data centers appeared, useful for load balancing, dynamic hardware upgrades, and elasticity of the services provided. The talk will present both the usage and the underlying techniques to achieve high performance and elasticity in modern Cloud Computing infrastructures.</p></blockquote><h4 id=platform-level-resource-orchestration-and-abstraction-layers-in-continuum-platforms-the-challenge-of-heterogeneity><strong>Platform-level resource orchestration and abstraction layers in continuum platforms: the challenge of heterogeneity</strong>,</h4><p>Massimo Coppola (ISTI, CNR)</p><blockquote><p>The talk will provide an overview of the approach to Continuum computing platforms brought on by the current research projects Accordion and TEACHING. Next-Generation applications call for a combination of computation, networking and storage-related QoS constraints that need to be guaranteed in spite of the volatility and changing behavior of all types of resources, of their heterogeneity, and of the dynamically changing mobility and needs of end-users. We&rsquo;ll discuss the approach to dynamic management of resources in platforms that encompass Clouds, Near- and Far-Edge devices, mobile and IoT devices, entailing the exploitation of different virtualization abstractions such as VMs, containers and unikernels, as well as the use of learning and AI techniques in order to characterize resource behavior.</p></blockquote><h4 id=scalable-containers-in-hpc-and-the-convergence-toward-hpc-clouds><strong>Scalable containers in HPC and the convergence toward HPC clouds</strong>,</h4><p>Azat Khuziyakhmetov (GWDG)</p><h4 id=pyxis---a-container-plugin-for-slurm><strong>Pyxis - a container plugin for Slurm</strong></h4><p>Luke Yaeger (NVIDIA)</p><h4 id=mpi-based-graph-computing-with-serverless-fault-tolerance-in-clouds><strong>MPI-based Graph Computing with Serverless Fault Tolerance in Clouds</strong></h4><p>Peng Lin and Lin Ma (ByteDance)</p><blockquote><p>Graph processing aims to process super large-scale graphs and execute graph data analysis tasks, e.g., PageRank, TriangleCount, community detection, etc. Most graph processing systems are under in-memory architecture, which makes it hard to process real-world large graphs with gigantic number of edges (e.g., 100 trillion edges) at a cheap cost. In addition, few graph processing systems really implement serverless fault tolerance capability, hindering wide adoption in production. Hence, we proposed an enterprise-level graph analytics platform for graph computing and graph mining in ByteDance. The system is built on a serverless engine atop Kubernetes to provide flexible cluster resource management, automatic deployment in cloud, elasticity for scalability and fault tolerance. It eases the maintenance and deployment effort, and lowers the number of machines and memory consumption but supports much larger graphs by a hierarchical out-of-core design of DRAM/PMEM/SSDs on Kubernetes. We also have built an actor-based control plane for rank management and stateful fault tolerance as an infrastructure component. It handles failures at container level, agent level, and worker level end-to-end with synergy that MPI can not fully cover. The agents of dynamically-assigned ranks manage worker processes of different languages as well as checkpoints in PMEM. It can relaunch agents and workers of specific ranks, load mutable status of vertices from vertex table in PMEM, and ensure that they are automatically recoverable from any iteration for any workers of arbitrary ranks.</p></blockquote><h3 id=general-information>General Information</h3><p>The <a href=https://vhpc.org>17th Workshop on Virtualization in High­-Performance Cloud Computing (VHPC 2022)</a> aims
to bring together researchers and industrial practitioners facing the
challenges posed by virtualization in HPC/Cloud scenarios
in order to foster discussion,
collaboration, mutual exchange of knowledge and experience, enabling research
to ultimately provide novel solutions for virtualized computing systems of
tomorrow.</p><p>The workshop will have a duration of one day and it will be held in conjunction with the
<a href=https://www.isc-hpc.com>International Supercomputing Conference - High Performance (ISC) 2022</a> on June 2nd, 2022 in
Hamburg, Germany.</p><p>For more information, refer to either the <a href=https://vhpc.org>VHPC 2022</a> or the <a href=https://www.isc-hpc.com>ISC 2022</a> web pages.</p></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://vhpc.github.io>&copy; 17th Workshop on Virtualization in High-Performance Cloud Computing 2022</a><div><div class=ananke-socials><a href=https://twitter.com/VHPC3 target=_blank class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel=noopener aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:{{ .fill }}"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" ><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></span></a></div></div></div></footer></body></html>